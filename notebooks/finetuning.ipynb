{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install necessary packages\n",
        "!pip install torch transformers timm einops datasets bitsandbytes accelerate roboflow supervision -q"
      ],
      "metadata": {
        "id": "WpgIMr-bC4jE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f73d4c-45c6-4e96-faaf-22152b778af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export ROBOFLOW_API_KEY='mFO2B5ou4cNfaHaFtl8e'"
      ],
      "metadata": {
        "id": "Tk7pOqymDQNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "F5o7QvrpDFwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('ROBOFLOW_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jeirs2-FFZik",
        "outputId": "26547974-cabb-4741-c829-f937e4612f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mFO2B5ou4cNfaHaFtl8e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Roboflow datasets"
      ],
      "metadata": {
        "id": "oAU4cSr0Qxxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use userdata.get(), you need to add your ROBOFLOW_API_KEY to the Colab secrets manager.\n",
        "# Click on the \"ğŸ”‘\" icon in the left sidebar, then click \"Add new secret\".\n",
        "# Enter 'ROBOFLOW_API_KEY' as the name and paste your API key as the value.\n",
        "rf = Roboflow(api_key=userdata.get('ROBOFLOW_API_KEY'))\n",
        "project = rf.workspace(\"think-tank\").project(\"311nyc\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco-segmentation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM5O6QC0QUxa",
        "outputId": "32bb9b33-c1d3-4b5d-9b03-4d8d2a5f008e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Exporting format coco-segmentation in progress : 85.0%\n",
            "Version export complete for coco-segmentation format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in 311NYC-2 to coco-segmentation:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1989/1989 [00:00<00:00, 11790.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to 311NYC-2 in coco-segmentation:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:00<00:00, 3527.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: create prompts and classes to finetune moondream\n"
      ],
      "metadata": {
        "id": "iLb0VPd6Rnos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "\n",
        "class RoboflowSegmentationDataset(Dataset):\n",
        "    def __init__(self, dataset_path, split):\n",
        "        self.split = split\n",
        "\n",
        "        images_dir = f\"{dataset_path}/{split}\"\n",
        "        ann_file  = f\"{images_dir}/_annotations.coco.json\"\n",
        "\n",
        "        # tell Supervision to forceâ€load all masks from the COCO segmentation annotations\n",
        "        self.dataset = sv.DetectionDataset.from_coco(\n",
        "            images_directory_path=images_dir,\n",
        "            annotations_path=ann_file,\n",
        "            force_masks=True,  # â† load the 'segmentation' field as binary masks :contentReference[oaicite:0]{index=0}\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        CLASSES = [\"\"]\n",
        "\n",
        "        # Retrieve the image/annotation info from the Supervision DetectionDataset\n",
        "        image_name, annotations = list(self.dataset.annotations.items())[idx]\n",
        "        image = self.dataset.images[image_name]\n",
        "\n",
        "        # Finds the amount of each type of violation is from the number of annotations there are\n",
        "        complaints = {}\n",
        "        for class_idx, complaint_type in enumerate(CLASSES):\n",
        "          count = len(annotations[annotations.class_id == (class_idx+1)]) # Counts the number of annotations with that class\n",
        "          if count == 0: continue;\n",
        "\n",
        "          complaints[complaint_type] = count\n",
        "\n",
        "        # Define the prompt/answer\n",
        "        prompt = f\"Pick the most appropriate label for this image... ({', '.join(CLASSES)}) Respond in JSON format.\"\n",
        "        answer = json.dumps(complaints, indent=2) # Formats the JSON and makes it the answer\n",
        "\n",
        "        # Return as the proper format\n",
        "        return {\n",
        "            \"image\": Image.fromarray(image),\n",
        "            \"qa\": [\n",
        "                {\n",
        "                    \"question\": prompt,\n",
        "                    \"answer\": answer,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "jzSWOxMLEJf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    \"train\": RoboflowDataset(dataset.location,\"train\"),\n",
        "    #\"valid\": RoboflowDataset(dataset.location,\"valid\"),\n",
        "    \"test\": RoboflowDataset(dataset.location,\"test\"),\n",
        "}\n",
        "\n",
        "# datasets = {\n",
        "#     \"test\": RoboflowDataset(dataset.location,\"test\"),\n",
        "# }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qF8xPMJ6VetZ",
        "outputId": "07d43842-a8c7-44a0-bb75-ebd90ce50a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RoboflowDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-379674421.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datasets = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRoboflowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#\"valid\": RoboflowDataset(dataset.location,\"valid\"),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRoboflowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RoboflowDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModel"
      ],
      "metadata": {
        "id": "om0lqmyHHu_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# On an M1 Mac, use MPS if available, otherwise CPU\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "# FlashAttention isnâ€™t supported on MPS or CPU\n",
        "FLASHATTENTION = None\n",
        "\n",
        "# MPS has partial float16 support but can be unstable; use float32 for safety\n",
        "DTYPE = torch.float32\n",
        "\n",
        "# Model revision\n",
        "MD_REVISION = \"2024-04-02\"\n",
        "\n",
        "print(f\"Using device={DEVICE}, dtype={DTYPE}, flash_attention={FLASHATTENTION}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3GQWdojIQlj",
        "outputId": "fea26959-c4bf-417b-dfd0-75253d42685b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device=cpu, dtype=torch.float32, flash_attention=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n",
        "moondream = AutoModelForCausalLM.from_pretrained(\n",
        "    \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n",
        "    attn_implementation=FLASHATTENTION,\n",
        "    torch_dtype=DTYPE, device_map={\"\": DEVICE}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEZ66fvfERO3",
        "outputId": "3598b88d-841b-4033-a11c-265fa608e758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model inputs: Get the image and question from the dataset sample.\n",
        "sample = datasets['test'][0]\n",
        "image = sample['image']\n",
        "question = sample['qa'][0]['question']\n",
        "\n",
        "print(\"Image and question successfully loaded from the dataset.\")"
      ],
      "metadata": {
        "id": "6nWjFIqZshxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "1ac5b224-6d4f-49e6-cfea-c91c946d1664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'datasets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-981167734.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare model inputs: Get the image and question from the dataset sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now call answer_question on the actual image + question\n",
        "md_answer = moondream.answer_question(\n",
        "    moondream.encode_image(image),\n",
        "    question,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# and plot the same image\n",
        "sv.plot_image(image, (3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "npxAGqaNE8X5",
        "outputId": "56a66ee2-a251-46aa-b2dd-b6f2e6cdd323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-2090181680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now call answer_question on the actual image + question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m md_answer = moondream.answer_question(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmoondream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of times to repeat the training dataset. Increasing this may cause the model to overfit or\n",
        "# lose generalization due to catastrophic forgetting. Decreasing it may cause the model to underfit.\n",
        "EPOCHS = 2\n",
        "\n",
        "# Number of samples to process in each batch. Set this to the highest value that doesn't cause an\n",
        "# out-of-memory error. Decrease it if you're running out of memory. Batch size 8 currently uses around\n",
        "# 15 GB of GPU memory during fine-tuning.\n",
        "BATCH_SIZE = 24\n",
        "\n",
        "# Number of batches to process before updating the model. You can use this to simulate a higher batch\n",
        "# size than your GPU can handle. Set this to 1 to disable gradient accumulation.\n",
        "GRAD_ACCUM_STEPS = 1\n",
        "\n",
        "# Learning rate for the Adam optimizer. Needs to be tuned on a case-by-case basis. As a general rule\n",
        "# of thumb, increase it by 1.4 times each time you double the effective batch size.\n",
        "#\n",
        "# Source: https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/\n",
        "#\n",
        "# Note that we linearly warm the learning rate up from 0.1 * LR to LR over the first 10% of the\n",
        "# training run, and then decay it back to 0.1 * LR over the last 90% of the training run using a\n",
        "# cosine schedule.\n",
        "LR = 3e-5\n",
        "\n",
        "# Whether to use Weights and Biases for logging training metrics.\n",
        "USE_WANDB = False\n"
      ],
      "metadata": {
        "id": "lvgw2V5lF2F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e89bcc90"
      },
      "source": [
        "## Task Complete\n",
        "\n",
        "We have successfully loaded the image and question from the dataset, encoded the image, formatted the prompt, prepared the inputs, generated an answer using the model's `generate` method, decoded the output, and displayed the results. This demonstrates how to use the `PhiForCausalLM` model directly with standard `transformers` library functions for image question answering, bypassing the custom `moondream.answer_question` method that was causing issues."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}